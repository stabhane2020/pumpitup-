{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fa882a",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4013c1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75a2f17",
   "metadata": {},
   "source": [
    "# Gradient Boosting Machines and the HistGradientBoostingClassifier\n",
    "Over the past few years, a class of models known as Gradient Boosting Machines (GBM) have become quite popular. One particular version/implementation known as [XGBoost](https://github.com/dmlc/xgboost) is commonly the technique of choice in winning Kaggle competitions. While these methods can seem quite mysterious and it's true that under the hood there are a lot of mathematical and technical details, the general idea behind them is pretty easy to grasp:\n",
    "\n",
    "* Build some model to predict something and make predictions\n",
    "* Compute the residuals from your first model (the errors)\n",
    "* Build a second model to predict *the first model's errors*\n",
    "* **Add** the second model's prediction to the first model's prediction (the boosting part)\n",
    "* ... rinse and repeat\n",
    "\n",
    "I emphased **Add** because boosting is an *additive* approach to modeling - we add a bunch of simple models together to make a more complex model.\n",
    "\n",
    "So, the big idea is iteratively improving the overall model by adding new models that try to better predict the previous model's errors. A good first read is this blog post, [Gradient Boosting Explained](https://www.gormanalysis.com/blog/gradient-boosting-explained/) by Ben Gorman (a Kaggle wizard). These techniques can be applied to both regression problems and classification problems. Then move on to the [brilliantly readable set of explanatory articles](https://explained.ai/gradient-boosting/) on GBM's done by Terrance Parr and Jeremy Howard.\n",
    "\n",
    "Even if you don't end up getting into the math details, it's good to keep in mind that when we say \"build a model\" we are implicitly saying that we need to do some sort of optimization to find the best model parameter values to minimize some *loss function* - a measure of overall error in our model predictions (for regression this might be MSE and for classification, [cross-entropy](https://en.wikipedia.org/wiki/Cross_entropy)). In these boosting models, the general optimization technique known as *stochastic gradient descent* often plays an important role. Hey, we came across this idea earlier when discussing the various solvers (e.g. 'saga') available in `LogisticRegression` in sklearn. So, developing some general intutition about how SGD works can help you understand and develop intuition about various machine learning techniques ranging from logistic regression to GBM's. We don't have the time to get into the details in this short summer course, but I will provide additional pointers to some very readable, and high quality, explanations, tutorials and demonstrations of these concepts. I'll do that at the end of this notebook.\n",
    "\n",
    "For now, let's just naively use a specific GBM that has recently been added to sklearn known as a [Histogram-based Gradient Boosting Classification Tree](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html?highlight=histgradient#sklearn.ensemble.HistGradientBoostingClassifier). Interestingly, this classifier is based on one called [LightGBM which was developed by Microsoft](https://github.com/Microsoft/LightGBM) and released as an open source project. As you'll see from the `import` statements below, it's still considered an experimental feature in sklearn. In order to use it, there were a few details that needed ironing out:\n",
    "\n",
    "* The target variable needs to be encoded using the [OrdinalEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html) though this seems to happen automatically (?).\n",
    "* The sklearn preprocessing steps we have been doing actually lead to the columns being reordered by column type (categorical vs numeric) and this led to our `categorical_cols_idx` list being wrong when we fed it to the `HistGradientBoostingClassifier`. You'll see comments in the code below that address issue.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8757d6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "# explicitly require this experimental feature\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "# now you can import normally from ensemble\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# Need the following to encode our target variable\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18bef07",
   "metadata": {},
   "source": [
    "Ok, let's use this thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0a3c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = pd.read_csv('./data/train_x.csv')\n",
    "train_y_raw = pd.read_csv(\"./data/raw/train_y.csv\")\n",
    "test_X = pd.read_csv(\"./data/test_x.csv\")\n",
    "\n",
    "# Drop id from train_y_raw\n",
    "train_y = train_y_raw.iloc[:, 1]\n",
    "\n",
    "# Create lists of columns by variable type\n",
    "target_col = ['status_group']\n",
    "categorical_cols = train_X.select_dtypes(include=['object']).columns.tolist()\n",
    "numeric_cols = train_X.select_dtypes(include=['number']).columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a2a09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57c7761",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer_hgbc = StandardScaler()\n",
    "categorical_transformer_hgbc = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.nan)\n",
    "target_transformer_hgbc = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.nan)\n",
    "\n",
    "preprocessor_hgbc = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer_hgbc, categorical_cols),\n",
    "        ('num', numeric_transformer_hgbc, numeric_cols)],\n",
    "        remainder='passthrough')\n",
    "\n",
    "# The preprocessor reorders the columns into blocks by type as defined by the transformers. So, now\n",
    "# the categorical cols are the first columns. We need their index numbers to use in the call to the\n",
    "# HistGradientBoostingClassifier() so it knows which cols to treat as categoricals (even though they\n",
    "# have been transformed into meaningless integers.)\n",
    "\n",
    "categorical_cols_idx = [_ for _ in range(len(categorical_cols))]\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "clf_hgbc = Pipeline(steps=[('preprocessor', preprocessor_hgbc),\n",
    "                      ('classifier', HistGradientBoostingClassifier(categorical_features=categorical_cols_idx))])\n",
    "\n",
    "# Fit model\n",
    "clf_hgbc.fit(train_X, train_y)\n",
    "print(f\"hgbc training score: {clf_hgbc.score(train_X, train_y):.3f}\")\n",
    "\n",
    "# Can just do the prediction using test data and predict will send it through the pipeline for preprocessing.\n",
    "clf_hgbc_test = clf_hgbc.predict(test_X)\n",
    "\n",
    "# Create submission file \n",
    "submit_dict = {'id': test_X['id'],\n",
    "              'status_group': clf_hgbc_test}\n",
    "\n",
    "hgbc_1_submission = pd.DataFrame(submit_dict, columns=['id', 'status_group'])\n",
    "hgbc_1_submission.to_csv('./output/hgbc_1_submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfb7f29",
   "metadata": {},
   "source": [
    "Well, hgbc did better than logistic model on the test data when I submitted it to Pump it Up (test accuracy = 0.7960). Still not as good as the random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e011ee9b",
   "metadata": {},
   "source": [
    "Let's look at confusion matrix to see where we are making mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885e8564",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(clf_hgbc, train_X, train_y, labels=clf_hgbc['classifier'].classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8036c584",
   "metadata": {},
   "source": [
    "Not surprisingly, the 'functional needs repair' status is the most difficult to predict correctly for this model as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6374725",
   "metadata": {},
   "source": [
    "# Ensembles\n",
    "Well, we've built three pretty different types of models: logistic regression, random forest, and a GBM. Why not bake them into a nice ensemble cake and see if we can improve our overall accuracy on the test data. Keep in mind that both random forests and GBM's are already ensemble approaches. So we are really making ensembles of ensembles!\n",
    "\n",
    "I saved the logistic regression and random forest models in a pickle file. Let's get them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bb6e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512e2496",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./output/models_clf_rf.pkl\", \"rb\") as model_file:\n",
    "    models_clf_rf = pickle.load(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26af613f",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_clf_rf.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf1b9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac4549d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create model\n",
    "ensemble_1 = VotingClassifier(estimators=[('clf_lr', models_clf_rf['clf_LR']),\n",
    "                                          ('clf_rf', models_clf_rf['clf_RF']),\n",
    "                                          ('clf_hgbc', clf_hgbc)], \n",
    "                              voting='soft', weights=[1.0, 1.0, 1.0])\n",
    "\n",
    "# 2. Fit model\n",
    "ensemble_1.fit(train_X, train_y)\n",
    "print(f\"Score: {ensemble_1.score(train_X, train_y):.4f}\")\n",
    "\n",
    "# 3. Predict\n",
    "clf_ensemble_1_test = ensemble_1.predict(test_X)\n",
    "\n",
    "# Create submission file \n",
    "submit_dict = {'id': test_X['id'],\n",
    "              'status_group': clf_ensemble_1_test}\n",
    "\n",
    "clf_ensemble_1_submission = pd.DataFrame(submit_dict, columns=['id', 'status_group'])\n",
    "clf_ensemble_1_submission.to_csv('./output/clf_ensemble_1_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f8f569",
   "metadata": {},
   "source": [
    "Hmmm, pretty high training score. Going to be interesting to see how we do on the contest test data...\n",
    "\n",
    "It scored 0.8104 in the actual competition - best submission so far. Lots of room for improvement and likely need to focus on variable selection, data prep and feature engineering. Unlikely that model tweaks are going to help."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e085f564",
   "metadata": {},
   "source": [
    "## Learning More\n",
    "\n",
    "### Gradient boosting machines\n",
    "\n",
    "If you want to learn more about GBM's and gradient descent, here's a few things you can do. Start by visiting Parr and Howard's explanation page for GBM's:\n",
    "\n",
    "* https://explained.ai/gradient-boosting/ - great portal to several readable overviews, background papers, and Jupyter notebooks\n",
    "\n",
    "They go on in a series of really nice posts to explain how GBM's work:\n",
    "\n",
    "* [Distance to Target](https://explained.ai/gradient-boosting/L2-loss.html) - Develops the intuition behind additive models (such as boosting) and works through a few examples to show how GBM's work.\n",
    "* [Heading in the right direction](https://explained.ai/gradient-boosting/L1-loss.html) - Takes a closer look at how GBMs work such as using the sign instead of the magnitude of the residual to guide the optimization.\n",
    "* [Gradient boosting performs gradient descent](https://explained.ai/gradient-boosting/descent.html) - They reveal that GBM's are really using *gradient descent*, a fundamental optimization technique and help you develop intution around how it works - if you're trying to get down to the bottom of valley quickly, in which direction do you start walking?\n",
    "\n",
    "The authors have created a series of Jupyter notebooks that go along with their tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32585d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
